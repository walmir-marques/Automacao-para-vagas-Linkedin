# Projeto de Web Scraping - Vagas de Emprego do LinkedIn

Este √© um projeto de **Web Scraping** que coleta vagas de emprego do **LinkedIn**, salva as informa√ß√µes em um banco de dados **PostgreSQL** hospedado no **Supabase**, e visa ser automatizado para rodar uma vez por dia, futuramente na **AWS**.

## üöÄ Tecnologias Utilizadas

- **Python**: Linguagem principal para desenvolver o script de scraping e interagir com o banco de dados.
- **BeautifulSoup**: Biblioteca para fazer o scraping da p√°gina web do LinkedIn e extrair as informa√ß√µes desejadas.
- **Supabase**: Plataforma para hospedar o banco de dados PostgreSQL, onde as vagas de emprego s√£o armazenadas.
- **PostgreSQL**: Banco de dados relacional utilizado para armazenar as vagas extra√≠das.
- **Datetime**: Biblioteca usada para registrar a data de inser√ß√£o das vagas no banco de dados.
- **Pandas**: Utilizado para manipula√ß√£o e organiza√ß√£o dos dados extra√≠dos em um formato estruturado antes de inseri-los no banco de dados.
- **AWS** (Futuro): A ideia √© colocar o projeto na AWS para realizar a automa√ß√£o di√°ria do processo de scraping.

## üìã Como Funciona

1. O **script de scraping** utiliza a biblioteca **BeautifulSoup** para acessar as p√°ginas de vagas de emprego no LinkedIn.
2. As informa√ß√µes extra√≠das incluem: t√≠tulo da vaga, nome da empresa, localiza√ß√£o, tempo de publica√ß√£o, link da vaga e da empresa, entre outras.
3. Esses dados s√£o armazenados em um **DataFrame** utilizando o **Pandas** e, posteriormente, inseridos no banco de dados **PostgreSQL** atrav√©s da integra√ß√£o com o **Supabase**.
4. O projeto possui uma l√≥gica de **upsert**, o que significa que, se a vaga j√° estiver cadastrada (identificada pelo link √∫nico da vaga), ela ser√° atualizada. Caso contr√°rio, uma nova vaga ser√° inserida.
5. **Futuro**: Uma vez por dia, o processo de scraping ser√° automatizado, hospedado na **AWS**, para garantir que os dados sejam sempre atualizados sem interven√ß√£o manual.

## üîß Como Rodar o Projeto

1. **Clone este reposit√≥rio**:
   ```bash
   git clone https://github.com/seu-usuario/nome-do-repositorio.git
   cd nome-do-repositorio
   
2. **Instale as depend√™ncias:**:
   ```bash
    pip install -r requirements.txt

3. **Configure sua chave de API do Supabase:**:
   - Crie uma conta no Supabase.
   - Crie um novo projeto e configure o banco de dados PostgreSQL
   - Obtenha a URL do projeto e a chava da API para autenticar sua conex√£o.

4. **Acesse os dados no Supabase:**: 
   ```bash
    python script_de_scraping.py

5. **Execute o script:**: Depois de executar o script, voc√™ poder√° ver as vagas salvas no seu banco de dados PostgreSQL.

6. ## üß† O Que Aprendi

- **Web Scraping com BeautifulSoup**: Como extrair dados estruturados de p√°ginas web de maneira eficiente e √©tica.
- **Integra√ß√£o com Supabase**: Como conectar Python ao Supabase para armazenar dados em um banco de dados PostgreSQL.
- **Automa√ß√£o de Processos**: A ideia de automatizar o scraping e armazenar dados de forma cont√≠nua, reduzindo a necessidade de interven√ß√£o manual.
- **Trabalhando com APIs**: Como usar a chave da API do Supabase para interagir com o banco de dados diretamente.
- **Melhoria cont√≠nua**: Como otimizar o c√≥digo para lidar com erros e garantir que as informa√ß√µes sejam sempre atualizadas de forma eficiente.

## ‚öôÔ∏è Planejamento Futuro

- **Automa√ß√£o na AWS**: No futuro, o projeto ser√° hospedado na AWS e configurado para rodar automaticamente uma vez por dia, utilizando o **AWS Lambda** ou **EC2** para executar o script de scraping.
- **Melhoria no Scraping**: Planejo melhorar o script para lidar com diferentes tipos de p√°ginas de vagas e otimizar o processo de extra√ß√£o de dados.
- **Escalabilidade**: A ideia √© escalar o projeto para coletar vagas de m√∫ltiplas fontes, al√©m do LinkedIn.

## üìÖ Melhorias Futuras

1. **Automa√ß√£o Completa**: Colocar o processo em execu√ß√£o di√°ria automaticamente usando **AWS Lambda** ou **Amazon EC2**.
2. **Scraping de M√∫ltiplos Sites**: Expandir o scraper para coletar vagas de outros sites de emprego, como **Glassdoor**, **Indeed**, entre outros.
3. **Interface de Visualiza√ß√£o**: Criar um painel para visualizar as vagas coletadas, talvez com **Dash** ou **Streamlit**, e disponibilizar uma interface para monitoramento.
4. **Notifica√ß√µes**: Configurar notifica√ß√µes por e-mail ou **Telegram** para alertar quando novas vagas forem inseridas.
