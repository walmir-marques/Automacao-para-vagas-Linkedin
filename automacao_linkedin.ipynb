{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraindo dados de vagas do Linkedin através de Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalando as bibliotecas necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
      "    #\n",
      "    ^\n",
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
      "    #\n",
      "    ^\n"
     ]
    }
   ],
   "source": [
    "!pip install requests # Usada para fazer requisições HTTP, ou seja, acessar páginas da web e pegar seu conteúdo.\n",
    "!pip install beautifulsoup4 # Usada para processar e interpretar o código HTML das páginas da web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando as Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando as bibliotecas necessárias\n",
    "\n",
    "import requests  # Para acessar páginas da web\n",
    "from bs4 import BeautifulSoup  # Para extrair informações do código HTML das páginas\n",
    "import pandas as pd  # Para criar e manipular tabelas de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coletando informações de filtros para a Vaga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recebendo o nome da vaga\n",
    "\n",
    "nome_vaga = input(\"Digite o nome da vaga: \")\n",
    "nome_vaga_tratada = nome_vaga.replace(\" \", \"%20\").lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recebendo a experiência da vaga\n",
    "\n",
    "experiencia = input(\"Digite o tipo de experiência: (0 = Todos, 1 = Estágio, 2 = Assistente, 3 = Júnior, 4 = Pleno-Sênior e 5 = Diretor)\")\n",
    "if experiencia == \"0\" or int(experiencia) > 5 or experiencia == \"\":\n",
    "    experiencia_tratada = ''\n",
    "else:\n",
    "    experiencia_tratada = f'''&f_E={experiencia}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recebendo o tipo da vaga\n",
    "\n",
    "tipo_vaga = input(\"Digite o tipo da vaga: (0 = Todos, 1 = Presencial, 2 = Remoto, 3 = Híbrido)\")\n",
    "if tipo_vaga == \"0\" or int(tipo_vaga) > 3 or tipo_vaga == \"\":\n",
    "    tipo_vaga_tratada = ''\n",
    "else:\n",
    "    tipo_vaga_tratada = f'''&f_WT={tipo_vaga}'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando a URL de busca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando as variáveis coletadas para criar a URL de pesquisa\n",
    "\n",
    "base_url = f'''\n",
    "https://www.linkedin.com/jobs/search?\n",
    "keywords={nome_vaga_tratada}\n",
    "&location=Brazil\n",
    "&geoId=106057199\n",
    "{experiencia_tratada}\n",
    "&f_TPR=r86400\n",
    "{tipo_vaga_tratada}\n",
    "&position=1\n",
    "&pageNum=0 \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazendo um tratamento para a URL \n",
    "\n",
    "base_url = base_url.replace(\"\\n\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fazendo a requisição para o Linkedin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faz uma requisição HTTP GET para a URL base e armazena a resposta na variável 'response'\n",
    "response = requests.get(base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraindo os dados da Página"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analisa o conteúdo HTML da resposta e cria um objeto BeautifulSoup para facilitar a extração de dados\n",
    "site = BeautifulSoup(response.text, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encontrando os elementos com informações das vagas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encontra todas as divs que possuem a classe específica usada para identificar os cartões de vagas no site\n",
    "dados = site.find_all(\"div\", attrs={\"class\": \"base-card relative w-full hover:no-underline focus:no-underline base-card--link base-search-card base-search-card--link job-search-card\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando uma lista para armazenar os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listas para armazenar os dados extraídos do site\n",
    "armazenando_vaga = []            # Armazena os títulos das vagas\n",
    "armazenamento_empresa = []        # Armazena os nomes das empresas\n",
    "armazenamento_localizacao = []    # Armazena as localizações das vagas\n",
    "armazenamento_tempo = []          # Armazena o tipo de contratação ou período (ex: tempo integral, meio período)\n",
    "armazenamento_data = []           # Armazena a data de publicação das vagas\n",
    "armazenamento_link_empresa = []   # Armazena os links para as páginas das empresas\n",
    "armazenamento_link_vaga = []      # Armazena os links diretos para as vagas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraindo informações de cada Vaga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de vagas armazenadas: 49\n"
     ]
    }
   ],
   "source": [
    "# Percorre cada elemento encontrado na busca dos cartões de vaga\n",
    "for palavra in dados:\n",
    "    # Nome da vaga\n",
    "    nome_vaga = palavra.find(\"h3\", attrs={\"class\": \"base-search-card__title\"})\n",
    "    nome_vaga = nome_vaga.text.strip() if nome_vaga else \"Sem Informação\"  # Remove espaços extras e trata casos vazios\n",
    "    armazenando_vaga.append(nome_vaga)  # Adiciona o nome da vaga à lista correspondente\n",
    "\n",
    "    # Nome da empresa\n",
    "    nome_empresa = palavra.find(\"h4\", attrs={\"class\": \"base-search-card__subtitle\"})\n",
    "    nome_empresa = nome_empresa.text.strip() if nome_empresa else \"Sem Informação\"\n",
    "    armazenamento_empresa.append(nome_empresa)\n",
    "\n",
    "    # Localização da vaga\n",
    "    localizacao = palavra.find(\"span\", attrs={\"class\": \"job-search-card__location\"})\n",
    "    localizacao = localizacao.text.strip() if localizacao else \"Sem Informação\"\n",
    "    armazenamento_localizacao.append(localizacao)\n",
    "\n",
    "    # Tempo/disponibilidade da vaga (ex: tempo integral, meio período)\n",
    "    tempo_vaga = palavra.find_all(\"time\")[0].text.strip()\n",
    "    armazenamento_tempo.append(tempo_vaga)\n",
    "\n",
    "    # Data da publicação da vaga\n",
    "    data = palavra.find_all(\"time\")[0]\n",
    "    data_trat = data[\"datetime\"]  # Obtém a data formatada diretamente do atributo 'datetime'\n",
    "    armazenamento_data.append(data_trat)\n",
    "\n",
    "    # Link direto para a vaga\n",
    "    link_vaga = palavra.find(\"a\", attrs={\"class\": \"base-card__full-link\"})\n",
    "    link_vaga = link_vaga.get(\"href\") if link_vaga else \"Sem Link\"\n",
    "    armazenamento_link_vaga.append(link_vaga)\n",
    "\n",
    "    # Link da página da empresa\n",
    "    link_empresa = palavra.find(\"a\", attrs={\"class\": \"hidden-nested-link\"})\n",
    "    link_empresa = link_empresa.get(\"href\") if link_empresa else \"Sem Link\"  # Corrigido para verificar a variável correta\n",
    "    armazenamento_link_empresa.append(link_empresa)\n",
    "\n",
    "# Exibe a quantidade total de vagas armazenadas\n",
    "print(f\"Total de vagas armazenadas: {len(armazenando_vaga)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando um dicionário com os dados coletados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criação de um dicionário para armazenar os dados coletados das vagas\n",
    "base_vagas = {\n",
    "    \"vaga\": armazenando_vaga,               # Lista com os títulos das vagas\n",
    "    \"empresa\": armazenamento_empresa,       # Lista com os nomes das empresas\n",
    "    \"localizacao\": armazenamento_localizacao,  # Lista com as localizações das vagas\n",
    "    \"tempo_postada\": armazenamento_tempo,   # Lista com o tempo desde a publicação da vaga\n",
    "    \"data_vaga\": armazenamento_data,        # Lista com a data exata de publicação da vaga\n",
    "    \"link_vaga\": armazenamento_link_vaga,   # Lista com os links diretos para as vagas\n",
    "    \"link_empresa\": armazenamento_link_empresa  # Lista com os links para as páginas das empresas\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando um dataframe para armazenar os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converte o dicionário 'base_vagas' em um DataFrame do Pandas para facilitar a análise e manipulação dos dados\n",
    "df = pd.DataFrame(base_vagas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proximos passos, Fazer a inclusão dos daos em um banco de dados e subir para o Supabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: supabase in c:\\users\\pradolux\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.13.0)\n",
      "Requirement already satisfied: gotrue<3.0.0,>=2.11.0 in c:\\users\\pradolux\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from supabase) (2.11.4)\n",
      "Requirement already satisfied: httpx<0.29,>=0.26 in c:\\users\\pradolux\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from supabase) (0.28.1)\n",
      "Requirement already satisfied: postgrest<0.20,>=0.19 in c:\\users\\pradolux\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from supabase) (0.19.3)\n",
      "Requirement already satisfied: realtime<3.0.0,>=2.0.0 in c:\\users\\pradolux\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from supabase) (2.4.0)\n",
      "Requirement already satisfied: storage3<0.12,>=0.10 in c:\\users\\pradolux\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from supabase) (0.11.3)\n",
      "Requirement already satisfied: supafunc<0.10,>=0.9 in c:\\users\\pradolux\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from supabase) (0.9.3)\n",
      "Requirement already satisfied: pydantic<3,>=1.10 in c:\\users\\pradolux\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from gotrue<3.0.0,>=2.11.0->supabase) (2.10.6)\n",
      "Requirement already satisfied: anyio in c:\\users\\pradolux\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<0.29,>=0.26->supabase) (4.8.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\pradolux\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<0.29,>=0.26->supabase) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\pradolux\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<0.29,>=0.26->supabase) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\users\\pradolux\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<0.29,>=0.26->supabase) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\pradolux\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpcore==1.*->httpx<0.29,>=0.26->supabase) (0.14.0)\n",
      "Requirement already satisfied: deprecation<3.0.0,>=2.1.0 in c:\\users\\pradolux\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from postgrest<0.20,>=0.19->supabase) (2.1.0)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.11.12 in c:\\users\\pradolux\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from realtime<3.0.0,>=2.0.0->supabase) (3.11.12)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.1 in c:\\users\\pradolux\\appdata\\roaming\\python\\python313\\site-packages (from realtime<3.0.0,>=2.0.0->supabase) (2.9.0.post0)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.12.2 in c:\\users\\pradolux\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from realtime<3.0.0,>=2.0.0->supabase) (4.12.2)\n",
      "Requirement already satisfied: websockets<15,>=11 in c:\\users\\pradolux\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from realtime<3.0.0,>=2.0.0->supabase) (14.2)\n",
      "Requirement already satisfied: strenum<0.5.0,>=0.4.15 in c:\\users\\pradolux\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from supafunc<0.10,>=0.9->supabase) (0.4.15)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\pradolux\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.11.12->realtime<3.0.0,>=2.0.0->supabase) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\pradolux\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.11.12->realtime<3.0.0,>=2.0.0->supabase) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\pradolux\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.11.12->realtime<3.0.0,>=2.0.0->supabase) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\pradolux\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.11.12->realtime<3.0.0,>=2.0.0->supabase) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\pradolux\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.11.12->realtime<3.0.0,>=2.0.0->supabase) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\pradolux\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.11.12->realtime<3.0.0,>=2.0.0->supabase) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\pradolux\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.11.12->realtime<3.0.0,>=2.0.0->supabase) (1.18.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\pradolux\\appdata\\roaming\\python\\python313\\site-packages (from deprecation<3.0.0,>=2.1.0->postgrest<0.20,>=0.19->supabase) (24.2)\n",
      "Requirement already satisfied: h2<5,>=3 in c:\\users\\pradolux\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx[http2]<0.29,>=0.26->gotrue<3.0.0,>=2.11.0->supabase) (4.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\pradolux\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3,>=1.10->gotrue<3.0.0,>=2.11.0->supabase) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\pradolux\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3,>=1.10->gotrue<3.0.0,>=2.11.0->supabase) (2.27.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\pradolux\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil<3.0.0,>=2.8.1->realtime<3.0.0,>=2.0.0->supabase) (1.17.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\pradolux\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from anyio->httpx<0.29,>=0.26->supabase) (1.3.1)\n",
      "Requirement already satisfied: hyperframe<7,>=6.1 in c:\\users\\pradolux\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from h2<5,>=3->httpx[http2]<0.29,>=0.26->gotrue<3.0.0,>=2.11.0->supabase) (6.1.0)\n",
      "Requirement already satisfied: hpack<5,>=4.1 in c:\\users\\pradolux\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from h2<5,>=3->httpx[http2]<0.29,>=0.26->gotrue<3.0.0,>=2.11.0->supabase) (4.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Instalando o Supabase\n",
    "!pip install supabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando as bibliotecas necessárias\n",
    "\n",
    "from supabase import Client, create_client  # Biblioteca para interagir com o banco de dados Supabase\n",
    "from datetime import datetime  # Biblioteca para trabalhar com datas e horários\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "supabase_url = os.getenv(\"SUPABASE_URL\")\n",
    "supabase_api_key = os.getenv(\"SUPABASE_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um cliente que se conecta ao Supabase com a URL e a API Key fornecidas,\n",
    "# e concedendo ao cliente permissões para fazer alterações no banco de dados\n",
    "supabase: Client = create_client(supabase_url, supabase_api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicionando uma nova coluna 'data_insercao' ao DataFrame, \n",
    "# com a data e hora atual formatada como 'YYYY-MM-DD' (ano-mês-dia)\n",
    "df[\"data_insercao\"] = datetime.now().strftime(\"%Y-%m-%d\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertendo o DataFrame 'df' para um dicionário, onde cada linha é representada como um dicionário,\n",
    "# com as colunas do DataFrame sendo as chaves e os valores das colunas sendo os valores correspondentes.\n",
    "# A opção 'orient=\"records\"' cria uma lista de dicionários, onde cada dicionário corresponde a uma linha.\n",
    "dados_vagas = df.to_dict(orient=\"records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 vagas foram inseridas com sucesso !!!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Tentando inserir os dados na tabela 'vagas' do Supabase\n",
    "    # O método 'upsert' tenta inserir os dados, mas se encontrar um conflito na chave primária (a coluna 'link_vaga'),\n",
    "    # ele irá atualizar os dados existentes ao invés de criar uma nova linha.\n",
    "    response_supabase = supabase.table(\"vagas\").upsert(dados_vagas, on_conflict=\"link_vaga\").execute()\n",
    "\n",
    "    # Verificando se os dados foram inseridos com sucesso\n",
    "    if response_supabase.data:\n",
    "        # Se a resposta contiver dados (vagas inseridas com sucesso)\n",
    "        print(f\"{len(dados_vagas)} vagas foram inseridas com sucesso !!!\")\n",
    "    else:\n",
    "        # Caso contrário, imprime o erro\n",
    "        print(f\"Erro ao tentar inserir a vaga: {response_supabase}\")\n",
    "except Exception as e:\n",
    "    # Captura qualquer erro que ocorra durante a operação e imprime uma mensagem de erro\n",
    "    print(\"Erro ao tentar conectar com o Supabase!!!\", str(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OS códigos abaixo foram feitos para estudo, criando um database local para testes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sqlite3\n",
    "# import pandas as pd\n",
    "# from datetime import datetime\n",
    "\n",
    "# # Criar a conexao com meu banco de dados SQLITE\n",
    "# nome_banco = \"vagas_linkedin2.db\"\n",
    "# conector = sqlite3.connect(nome_banco)\n",
    "# cursor = conector.cursor()\n",
    "\n",
    "# # criando nossa tabela para armazenar as vagas\n",
    "\n",
    "# cursor.execute('''\n",
    "# CREATE TABLE IF NOT EXISTS vagas(\n",
    "# id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "# vaga TEXT NOT NULL,\n",
    "# empresa TEXT NOT NULL,\n",
    "# localizacao TEXT NOT NULL,\n",
    "# Tempo_postada TEXT NOT NULL,\n",
    "# data_vaga DATE NOT NULL,\n",
    "# link_vaga TEXT UNIQUE NOT NULL,\n",
    "# link_empresa TEXT NOT NULL,\n",
    "# data_insercao DATE NOT NULL\n",
    "# )\n",
    "# ''')\n",
    "\n",
    "# # Adicionando data no Python\n",
    "# data_hoje = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "# df[\"data_insercao\"] = data_hoje\n",
    "\n",
    "# dados_linkedin = list(df.itertuples(index = False, name = None))\n",
    "\n",
    "# cursor.executemany('''\n",
    "#   INSERT OR IGNORE INTO vagas (vaga, empresa, localizacao, Tempo_postada, data_vaga, link_vaga, link_empresa, data_insercao)\n",
    "#   VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "# ''', dados_linkedin)\n",
    "\n",
    "# # salvar as infos no banco\n",
    "# conector.commit()\n",
    "\n",
    "# # fechar conexao\n",
    "# conector.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
